# Overview

This is an educational LLM whose purpose is to answer question on the data you want. 


# Usage

## Ollama with Mistral

Install Ollama locally using `curl https://ollama.ai/install.sh | sh`
Then from the CLI to run the model `mistral` run `ollama run mistal`